{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest & Decision Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionary for the dataset name & paths \n",
    "DATASETS ={\n",
    "    \"small\":\"../Data/small/glass.data\",\n",
    "    \"medium\":\"../Data/medium/drug_consumption.data\",\n",
    "    \"large\":\"../Data/large/c2k_data_comma.csv\"\n",
    "               }\n",
    "\n",
    "## Data Split Parameters \n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 1 - TRAIN_SIZE\n",
    "\n",
    "## Random Forest Parameters\n",
    "NUM_TREES = 100 ## 1,10,25,50,75,100 \n",
    "NUM_FEATURES = 10  ## 1,3, int(log_2(M)+1), sqrt(M), where M is the number of features\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the dictionary.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'small':\n",
    "        ## exclude the first column ID (labeled 0 to 10, so use 1-10)\n",
    "        df = pd.read_csv(DATASETS[dataset_name],header=None)\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "        df.columns = list(range(0,10))\n",
    "        return df\n",
    "    \n",
    "def split_train_test(data: pd.DataFrame, train_size: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets.\n",
    "    \"\"\"\n",
    "    train_data = data.sample(frac=train_size, random_state=RANDOM_STATE)\n",
    "    test_data = data.drop(train_data.index)\n",
    "    return train_data, test_data\n",
    "\n",
    "nb_train = int(np.floor(0.9 * len(df)))\n",
    "df = df.sample(frac=1, random_state=217)\n",
    "X_train = df[features][:nb_train]\n",
    "y_train = df['Survived'][:nb_train].values\n",
    "X_test = df[features][nb_train:]\n",
    "y_test = df['Survived'][nb_train:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data \n",
    "small = load_dataset(\"small\")\n",
    "TARGET_COL = small.columns[-1]\n",
    "## split the data \n",
    "train, test = split_train_test(small, TRAIN_SIZE)\n",
    "## split the train into features and labels\n",
    "nb_train = int(np.floor(0.9 * len(train)))\n",
    "X_train, y_train = train.drop(TARGET_COL, axis=1)[:nb_train], train[TARGET_COL][:nb_train]\n",
    "X_test, y_test = test.drop(TARGET_COL, axis=1)[nb_train:], test[TARGET_COL][nb_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data:np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the entropy given the following formula: \n",
    "    H(x) = -sum_j(p_j * log(p_j))\n",
    "    \"\"\"\n",
    "    ## get the unique counts of the array \n",
    "    _, counts = np.unique(data, return_counts=True)\n",
    "    ## get the probabilities\n",
    "    probas = counts / data.shape[0]\n",
    "    if np.sum(probas) == 0:\n",
    "        return 0 \n",
    "    elif np.sum(probas) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        probas = probas + 1e-7\n",
    "        ## add the 1e-7 to avoid log(0)\n",
    "        return np.sum(-probas * np.log2(probas))/float(data.shape[0])\n",
    "    \n",
    "def information_gain(left_branch, right_branch) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the information gain given the following formula:\n",
    "    G(x) = H(x) - sum_j(p_j * H(x|j))\n",
    "    \"\"\"\n",
    "    p = len(left_branch) / (len(left_branch) + len(right_branch))\n",
    "    return entropy(left_branch) + entropy(right_branch) - p * entropy(left_branch) - (1 - p) * entropy(right_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(X_train:np.array, y_train:np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Function which gets the bootstrap samples.\n",
    "    These are samples WITH replacement, thus, the left over samples are \n",
    "    Left_Over = lim_x_to_inf [(1 - 1/n)^n] -> e^-1 ~ 1/3\n",
    "    So for the fitting 2/3 of the observations will be used. \n",
    "    \n",
    "    PARAMS:\n",
    "    -------\n",
    "    X_train : training samples of the dataset \n",
    "    \n",
    "    y_train : training labels of the dataset \n",
    "    \n",
    "    RETURNS:\n",
    "    --------\n",
    "    X_bootstrap : bootstrap samples for X_train \n",
    "    \n",
    "    y_bootstrap : bootstrap samples for y_train \n",
    "    \n",
    "    x_oob : out of bag samples for X_train \n",
    "    \n",
    "    y_oob : out of bag samples for y_train \n",
    "    \n",
    "    \"\"\"\n",
    "    ## first get the indices of the bootstrap samples \n",
    "    bootstrap_idx = np.random.choice(range(X_train.shape[0]), size=(X_train.shape[0]), replace=True)\n",
    "    ## get the bootstrap samples\n",
    "    x_bootstrap = X_train.loc[X_train.index.intersection(bootstrap_idx)].values\n",
    "    y_bootstrap = y_train[y_train.index.intersection(bootstrap_idx)]\n",
    "    return x_bootstrap, y_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the gini function \n",
    "def gini(data: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the gini impurity given the following formula:\n",
    "    G(x) = 1 - sum_j(p_j^2)\n",
    "    \"\"\"\n",
    "    ## get the unique counts of the array \n",
    "    _, counts = np.unique(data, return_counts=True)\n",
    "    ## get the probabilities\n",
    "    probas = counts / data.shape[0]\n",
    "    return 1 - np.sum(probas ** 2)\n",
    "\n",
    "## define a function to calculate the OOB error\n",
    "def out_of_bag_error(x_test:np.array, y_test:np.array, model:object) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the out of bag error given the following formula:\n",
    "    E_OOB = 1/n * sum_i(1 - p(x_i|y_i))\n",
    "    \"\"\"\n",
    "    ## get the predictions for each item in the test set\n",
    "    preds = np.array([predict_samples(model, i) for i in x_test])\n",
    "    ## get the sum of the mismatches \n",
    "    mismatch = np.sum(preds != y_test)\n",
    "    ## return the error\n",
    "    return mismatch / y_test.shape[0]\n",
    "\n",
    "## define a function to find the best split point \n",
    "## it should select m features at random \n",
    "## for each feature in the bootstrapped samples, it calculates the information gain\n",
    "## returns a dictionary with: feature index, split value, left_branch, right_branch\n",
    "def best_split_finder(X_bootstrap: np.array, y_bootstrap: np.array, max_features_to_use: int) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the best split for each feature in the bootstrapped samples.\n",
    "    \n",
    "    PARAMS: \n",
    "    -------\n",
    "    X_bootstrap : training bootstrapped samples \n",
    "    \n",
    "    y_bootstrap : training labels boostrapped samples\n",
    "    \n",
    "    max_features_to_use : (F) the number of features to consider when doing the splits\n",
    "    \n",
    "    RETURNS:\n",
    "    -------\n",
    "    best_split : dictionary with the index of the feature, the split value, and the right & left nodes\n",
    "    \n",
    "    \"\"\"\n",
    "    ## get the number of features which are in the bootstrap samples \n",
    "    #n_features = X_bootstrap.shape[1]\n",
    "    n_features = len(X_bootstrap[0])\n",
    "    ## n_features is the the length of the bootstrap if there are two dimensions else its the length of the first dimension\n",
    "    ## keep a holder for the features which were used \n",
    "    feature_holder = np.array([],dtype=np.float32)\n",
    "    ## while the length of the feature holder is less than the max features to use\n",
    "    ## keep adding features to the feature holder\n",
    "    ## this is to ensure that we don't use the same feature twice\n",
    "    while len(feature_holder) <= max_features_to_use:\n",
    "        feature_to_add = np.random.choice(range(n_features)) ## defaults to 1\n",
    "        ## check if the feature is not in the holder \n",
    "        if feature_to_add not in feature_holder:\n",
    "            #feature_holder.append(feature_to_add)\n",
    "            ## add it to the array \n",
    "            feature_holder = np.append(feature_holder, feature_to_add)\n",
    "    ## start calculating the information gain for each feature \n",
    "    ## keep track of the best information gain & the current node \n",
    "    top_information_gain = -999999\n",
    "    feat_idx = None\n",
    "    left_child_x,right_child_x = None, None\n",
    "    left_child_y,right_child_y = None, None\n",
    "    ## iterate over the features at their indices\n",
    "    for f_idx in feature_holder:\n",
    "        f_idx = int(f_idx)\n",
    "        for best_split_point in X_bootstrap[:, f_idx]:\n",
    "            ## define the children & their bootstrap \n",
    "            right_child_x_bootstrap, right_child_y_bootstrap = np.array([],dtype=np.float32),np.array([],dtype=np.float32)\n",
    "            left_child_x_bootstrap,left_child_y_bootstrap = np.array([],dtype=np.float32),np.array([],dtype=np.float32)\n",
    "            ## enumerate the bootstrap samples\n",
    "            for split_idx, boot_val in enumerate(X_bootstrap[:, f_idx]):\n",
    "                ## check if the value is less than the split point\n",
    "                if boot_val <= best_split_point:\n",
    "                    ## add it to the left child bootstrap\n",
    "                    left_child_x_bootstrap = np.append(left_child_x_bootstrap, X_bootstrap[split_idx])\n",
    "                    left_child_y_bootstrap = np.append(left_child_y_bootstrap, y_bootstrap.iloc[split_idx])\n",
    "                else:\n",
    "                    ## add it to the right child bootstrap\n",
    "                    right_child_x_bootstrap = np.append(right_child_x_bootstrap, X_bootstrap[split_idx])\n",
    "                    right_child_y_bootstrap = np.append(right_child_y_bootstrap, y_bootstrap.iloc[split_idx])\n",
    "            ## calculate the information gain for the current split\n",
    "            curr_split_gain = information_gain(left_child_y_bootstrap, right_child_y_bootstrap)\n",
    "            ## check if the current split is better than the current node\n",
    "            if curr_split_gain > top_information_gain:\n",
    "                ## update the top information gain\n",
    "                top_information_gain = curr_split_gain\n",
    "                ## update the current node\n",
    "                # curr_node = {\n",
    "                #     'information_gain': top_information_gain,\n",
    "                #     'feature_idx': f_idx,\n",
    "                #     'split_val': best_split_point,\n",
    "                #     'left_child': left_child_x_bootstrap,\n",
    "                #     'right_child': right_child_x_bootstrap\n",
    "                #              }\n",
    "                feat_idx = f_idx\n",
    "                left_child_x = left_child_x_bootstrap\n",
    "                right_child_x = right_child_x_bootstrap\n",
    "                left_child_y = left_child_y_bootstrap\n",
    "                right_child_y = right_child_y_bootstrap\n",
    "    \n",
    "    node_dict = {\n",
    "                'information_gain': top_information_gain,\n",
    "                'feature_idx': feat_idx,\n",
    "                'split_val': best_split_point,\n",
    "                'left_child': {\"X_bootstrap\":left_child_x, \"y_bootstrap\":left_child_y},\n",
    "                'right_child': {\"X_bootstrap\":right_child_x, \"y_bootstrap\":right_child_y}\n",
    "                            }\n",
    "    \n",
    "    return node_dict \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b, y_b = get_bootstrap_samples(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = best_split_finder(x_b, y_b, max_features_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define function for a terminal node \n",
    "def terminal_node(node:dict) -> int:\n",
    "    ## get the y bootstrap \n",
    "    y_boot = node['y_bootstrap']\n",
    "    return max(y_boot, key=y_boot.count)\n",
    "\n",
    "## define a function to split the node \n",
    "def node_splitted(node: dict,\n",
    "                  max_features:int,\n",
    "                  min_samples_split:int,\n",
    "                  max_depth:int,\n",
    "                  depth:int) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the node is splitted or not.\n",
    "    \n",
    "    PARAMS:\n",
    "    -------\n",
    "    node : the node to check if it is splitted or not\n",
    "    \n",
    "    max_features : the number of features to consider when doing the splits\n",
    "    \n",
    "    min_samples_split : the minimum number of samples to split a node\n",
    "    \n",
    "    max_depth : the maximum depth to split a node\n",
    "    \n",
    "    depth : the current depth of the node\n",
    "    \n",
    "    RETURNS:\n",
    "    --------\n",
    "    None : splits node of the tree \n",
    "    \n",
    "    \"\"\"\n",
    "    ## get the children \n",
    "    left, right = node['left_child'], node['right_child']\n",
    "    ## delete the nodes \n",
    "    del(node['left_child'])\n",
    "    del(node['right_child'])\n",
    "    \n",
    "    ## check if the bootstrap is empty, for either child \n",
    "    if len(left['y_bootstrap']) == 0 or len(right['y_bootstrap']) == 0:\n",
    "        ## create the empty node, which is equal to the sum of the children's y_boot\n",
    "        empty_child_node = {\"y_bootstrap\":left['y_bootstrap'] + right['y_bootstrap']}\n",
    "        ## add the empty node to the left & right child\n",
    "        node['left_child'] = terminal_node(empty_child_node)\n",
    "        node['right_child'] = terminal_node(empty_child_node)\n",
    "        return \n",
    "    ## check if the depth is greater than the max depth\n",
    "    if depth >= max_depth:\n",
    "        ## set the node to a terminal node \n",
    "        node['left_child'] = terminal_node(left)\n",
    "        node['right_child'] = terminal_node(right)\n",
    "        return node\n",
    "    ## check if the number of samples is less than the min samples split\n",
    "    if len(left['X_bootstrap']) <= min_samples_split:\n",
    "        ## set the node to a terminal node \n",
    "        node['split_left'] = node['split_right'] = terminal_node(left)\n",
    "    else:\n",
    "        #return left['X_bootstrap'], left['y_bootstrap']\n",
    "        ## find the split point for the node\n",
    "        node['split_left'] = best_split_finder(left['X_bootstrap'], left['y_bootstrap'], max_features_to_use=max_features)\n",
    "        ## split the nodes \n",
    "        node_splitted(node['split_left'], max_features, min_samples_split, max_depth, depth+1)\n",
    "    ## the same for the right node \n",
    "    if len(right['X_bootstrap']) <= min_samples_split:\n",
    "        ## set the node to a terminal node \n",
    "        node['split_right'] = node['split_left'] = terminal_node(left)\n",
    "    else:\n",
    "        ## find the split point for the node \n",
    "        node['split_right'] = best_split_finder(right['X_bootstrap'], right['y_bootstrap'], max_features_to_use=max_features)\n",
    "        ## split the nodes \n",
    "        node_splitted(node['split_right'], max_features, min_samples_split, max_depth, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8751/1035367132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m node_splitted(n_c, max_features=1,\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8751/1919011454.py\u001b[0m in \u001b[0;36mnode_splitted\u001b[0;34m(node, max_features, min_samples_split, max_depth, depth)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#return left['X_bootstrap'], left['y_bootstrap']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m## find the split point for the node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split_left'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_split_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_bootstrap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_bootstrap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features_to_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m## split the nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnode_splitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split_left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bd/f5z4cc193xgdxq1yr1xpflmm0000gn/T/ipykernel_8751/2011366822.py\u001b[0m in \u001b[0;36mbest_split_finder\u001b[0;34m(X_bootstrap, y_bootstrap, max_features_to_use)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m## get the number of features which are in the bootstrap samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#n_features = X_bootstrap.shape[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bootstrap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m## n_features is the the length of the bootstrap if there are two dimensions else its the length of the first dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m## keep a holder for the features which were used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "n_c = copy.copy(node)\n",
    "node_splitted(n_c, max_features=1,\n",
    "              min_samples_split=2,\n",
    "              max_depth=10,\n",
    "              depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6858c3dbc49267e902ff986705b591b9d7b57befff84fd7d814fe16c4a8e1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
