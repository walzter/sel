{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from termcolor import colored\n",
    "import itertools\n",
    "\n",
    "## helper functions for the tree\n",
    "from helpers import *\n",
    "\n",
    "def make_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets the seed for the random number generator.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def make_standard_sample(X_train: np.array, y_train: np.array) -> np.array:\n",
    "    xboot, yboot = (\n",
    "        X_train.values,\n",
    "        y_train,\n",
    "    )\n",
    "    return xboot, yboot\n",
    "\n",
    "\n",
    "def split_multiple_nodes(\n",
    "    xboot: np.array, yboot: np.array, top_features: int\n",
    ") -> dict:\n",
    "    \"\"\" \"\"\"\n",
    "    all_features = []\n",
    "    num_features = len(xboot[0])\n",
    "    while len(all_features) <= top_features:\n",
    "        fidx = random.sample(range(num_features), 1)\n",
    "        if fidx not in all_features:\n",
    "            all_features.extend(fidx)\n",
    "    top_score = -9.99 * 1000\n",
    "    curr_node = None\n",
    "    for feature_idx in all_features:\n",
    "        for splitter in xboot[:, feature_idx]:\n",
    "            lc = {\"xboot\": [], \"yboot\": []}\n",
    "            rc = {\"xboot\": [], \"yboot\": []}\n",
    "\n",
    "            for i, value in enumerate(xboot[:, feature_idx]):\n",
    "                if value <= splitter:\n",
    "                    lc[\"xboot\"].append(xboot[i])\n",
    "                    lc[\"yboot\"].append(yboot[i])\n",
    "                else:\n",
    "                    rc[\"xboot\"].append(xboot[i])\n",
    "                    rc[\"yboot\"].append(yboot[i])\n",
    "\n",
    "            curr_info_gain = information_gain(\n",
    "                lc[\"yboot\"], rc[\"yboot\"]\n",
    "            )\n",
    "            if curr_info_gain > top_score:\n",
    "                top_score = curr_info_gain\n",
    "                lc[\"xboot\"] = np.array(lc[\"xboot\"])\n",
    "                rc[\"xboot\"] = np.array(rc[\"xboot\"])\n",
    "                curr_node = {\n",
    "                    \"information_gain\": curr_info_gain,\n",
    "                    \"left_child\": lc,\n",
    "                    \"right_child\": rc,\n",
    "                    \"split_point\": splitter,\n",
    "                    \"feature_idx\": feature_idx,\n",
    "                }\n",
    "\n",
    "    return curr_node\n",
    "\n",
    "\n",
    "def leaf(node: dict) -> int:\n",
    "    \"\"\"\n",
    "    Returns a prediction of a class for a node.\n",
    "    \"\"\"\n",
    "    return max(node[\"yboot\"], key=node[\"yboot\"].count)\n",
    "\n",
    "\n",
    "def split_single_node(\n",
    "    node: dict, top_features: int, min_obs: int, tiefe: int, depth: int\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Checks if the node is splitted or not.\n",
    "\n",
    "    PARAMS:\n",
    "    -------\n",
    "    node : the node to check if it is splitted or not\n",
    "\n",
    "    top_features : the number of features to consider when doing the splits\n",
    "\n",
    "    min_obs : the minimum number of samples to split a node\n",
    "\n",
    "    tiefe : the maximum depth to split a node\n",
    "\n",
    "    depth : the current depth of the node\n",
    "\n",
    "    RETURNS:\n",
    "    --------\n",
    "    None : splits node of the tree\n",
    "\n",
    "    \"\"\"\n",
    "    lc, rc = node[\"left_child\"], node[\"right_child\"]\n",
    "\n",
    "    del node[\"left_child\"]\n",
    "    del node[\"right_child\"]\n",
    "    \n",
    "    if len(lc[\"yboot\"]) == 0 or len(rc[\"yboot\"]) == 0:\n",
    "        empty_child = {\"yboot\": lc[\"yboot\"] + rc[\"yboot\"]}\n",
    "        node[\"left_split\"] = leaf(empty_child)\n",
    "        node[\"right_split\"] = leaf(empty_child)\n",
    "        return\n",
    "\n",
    "    if depth >= tiefe:\n",
    "        node[\"left_split\"] = leaf(lc)\n",
    "        node[\"right_split\"] = leaf(rc)\n",
    "        return node\n",
    "\n",
    "    if len(lc[\"xboot\"]) <= min_obs:\n",
    "        node[\"left_split\"] = node[\"right_split\"] = leaf(lc)\n",
    "    else:\n",
    "        node[\"left_split\"] = split_multiple_nodes(\n",
    "            lc[\"xboot\"], lc[\"yboot\"], top_features\n",
    "        )\n",
    "        split_single_node(\n",
    "            node[\"left_split\"], tiefe, min_obs, tiefe, depth + 1\n",
    "        )\n",
    "    if len(rc[\"xboot\"]) <= min_obs:\n",
    "        node[\"right_split\"] = node[\"left_split\"] = leaf(rc)\n",
    "    else:\n",
    "        node[\"right_split\"] = split_multiple_nodes(\n",
    "            rc[\"xboot\"], rc[\"yboot\"], top_features\n",
    "        )\n",
    "        split_single_node(\n",
    "            node[\"right_split\"], top_features, min_obs, tiefe, depth + 1\n",
    "        )\n",
    "\n",
    "\n",
    "def grow_a_tree(\n",
    "    xboot: np.array,\n",
    "    yboot: np.array,\n",
    "    tiefe: int,\n",
    "    min_obs: int,\n",
    "    top_features: int,\n",
    "):\n",
    "    base_ = split_multiple_nodes(xboot, yboot, top_features)\n",
    "    split_single_node(base_, top_features, min_obs, tiefe, 1)\n",
    "    return base_\n",
    "\n",
    "\n",
    "def rf_model(\n",
    "    X_train: np.array,\n",
    "    y_train: np.array,\n",
    "    n_estimators: int,\n",
    "    top_features: int,\n",
    "    tiefe: int = 10,\n",
    "    min_obs: int = 2,\n",
    "):\n",
    "    all_trees = [None] * n_estimators\n",
    "    for i in range(n_estimators):\n",
    "        xboot, yboot = make_standard_sample(X_train, y_train)\n",
    "        tree = grow_a_tree(\n",
    "            xboot, yboot, top_features, tiefe, min_obs\n",
    "        )\n",
    "        all_trees[i] = tree\n",
    "    return all_trees\n",
    "\n",
    "\n",
    "def make_prediction(tree, X_test):\n",
    "    fidx = tree[\"feature_idx\"]\n",
    "\n",
    "    if X_test[fidx] <= tree[\"split_point\"]:\n",
    "        if type(tree[\"left_split\"]) == dict:\n",
    "            return make_prediction(tree[\"left_split\"], X_test)\n",
    "        else:\n",
    "            value = tree[\"left_split\"]\n",
    "            return value\n",
    "    else:\n",
    "        if type(tree[\"right_split\"]) == dict:\n",
    "            return make_prediction(tree[\"right_split\"], X_test)\n",
    "        else:\n",
    "            return tree[\"right_split\"]\n",
    "\n",
    "\n",
    "def predictor(all_trees: list, X_test: np.array) -> np.array:\n",
    "    rf_predictions = []\n",
    "    for observation in range(len(X_test)):\n",
    "        all_preds = [\n",
    "            make_prediction(tree, X_test.values[observation]) for tree in all_trees\n",
    "        ]\n",
    "        final_pred = max(all_preds, key=all_preds.count)\n",
    "        rf_predictions.append(final_pred)\n",
    "    return np.array(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mModel Decision_Forest | Trees 1 | Number of Features 42 | Accuracy 34.88 | Time 2.64s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 1 | Number of Features 85 | Accuracy 34.88 | Time 6.29s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 1 | Number of Features 128 | Accuracy 37.21 | Time 14.48s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 1 | Number of Features 103 | Accuracy 27.91 | Time 4.44s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 10 | Number of Features 42 | Accuracy 30.23 | Time 23.89s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 10 | Number of Features 85 | Accuracy 34.88 | Time 46.20s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 10 | Number of Features 128 | Accuracy 27.91 | Time 72.23s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 10 | Number of Features 103 | Accuracy 23.26 | Time 64.62s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 25 | Number of Features 42 | Accuracy 27.91 | Time 92.64s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 25 | Number of Features 85 | Accuracy 32.56 | Time 160.91s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 25 | Number of Features 128 | Accuracy 32.56 | Time 240.81s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 25 | Number of Features 103 | Accuracy 37.21 | Time 212.99s\u001b[0m\n",
      "\u001b[32mModel Decision_Forest | Trees 50 | Number of Features 42 | Accuracy 30.23 | Time 167.04s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "make_seed(SEED)\n",
    "def runif(M):\n",
    "    return np.random.randint(low=1,high=M+1)\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-ds\",type=str, help=\"dataset\")\n",
    "# args = parser.parse_args()\n",
    "## include the argparse\n",
    "DATA_SET_SIZE = 'small'\n",
    "X_train, y_train, X_test, y_test = prepare_dataset(DATA_SET_SIZE)\n",
    "## NAME FOR THE FILE \n",
    "NAME = \"Decision_Forest\"\n",
    "## NUMBER OF TREES \n",
    "NUM_TREES = [1, 10, 25, 50, 75, 100]\n",
    "## NUMBER OF FEATURES \n",
    "M = len(X_train)\n",
    "F = [int(M/4), int(M/2), int(M*3/4), runif(M)]\n",
    "## make combinations of all the parameters\n",
    "combs = list(itertools.product(NUM_TREES, F))\n",
    "## run the model for each combination\n",
    "holders = []\n",
    "for nt, f in combs:\n",
    "    ips = [(X_train, y_train, nt, f)]\n",
    "    tic = time.time()\n",
    "    model = rf_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        n_estimators=nt,\n",
    "        top_features=f,\n",
    "        tiefe=10,\n",
    "        min_obs=2\n",
    "    )\n",
    "    toc = time.time()\n",
    "    tic_toc = toc - tic\n",
    "    preds = predictor(model, X_test)\n",
    "    acc = sum(preds == y_test) / len(y_test)\n",
    "    ## print the results: Trees | Features | Accuracy | Time , in green\n",
    "    print(\n",
    "        colored(\n",
    "            f\"Model {NAME} | Trees {nt} | Number of Features {f} | Accuracy {acc*100:.2f} | Time {tic_toc:.2f}s\",\n",
    "            \"green\",\n",
    "        )\n",
    "    )\n",
    "    ## save the results to a csv file\n",
    "    model_df = pd.DataFrame.from_records(model)\n",
    "    model_df.drop([\"left_split\", \"right_split\"], axis=1, inplace=True)\n",
    "    ## add the columns for nt & f\n",
    "    model_df[\"Num_trees\"] = nt\n",
    "    model_df[\"Num_features\"] = f\n",
    "    model_df[\"Accuracy\"] = acc\n",
    "    holders.append(model_df)\n",
    "    ## combine the list of dataframes\n",
    "combined_df = pd.concat(holders)\n",
    "combined_df.to_csv(f\"./Data/out/{DATA_SET_SIZE}_{NAME}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6858c3dbc49267e902ff986705b591b9d7b57befff84fd7d814fe16c4a8e1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
